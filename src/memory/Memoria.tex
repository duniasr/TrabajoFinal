\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}

% Márgenes
\geometry{top=2.5cm, bottom=2.5cm, left=3cm, right=3cm}

% Colores para código
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Encabezado y pie de página
\pagestyle{fancy}
\fancyhf{}
\rhead{Memorizador - Simón Dice}
\lhead{Visión por Computador}
\cfoot{\thepage}

\begin{document}

% PORTADA
\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    \includegraphics[width=0.4\textwidth]{../assets/eii-acron-mod.png} 
    \vspace{1.5cm}
    
    {\Large Grado en Ingeniería Informática}\\[0.5cm]
    {\Large Asignatura: Visión por Computador}
    
    \vspace{2cm}
    
    {\Huge \textbf{TRABAJO FINAL DE ASIGNATURA}}\\[0.5cm]
    {\Huge \textit{Simon Dice}}
    
    \vspace{2cm}
    
    \textbf{Autores:}\\
    Laura Herrera Negrín\\
    Dunia Suárez Rodríguez\\
    Ayman Asbai Ghoudan
    
    \vspace{2cm}
    
    \today
    
\end{titlepage}

\tableofcontents
\newpage

% 1. INTRODUCCIÓN
\section{Introducción}

\subsection{Descripción del proyecto}
El presente proyecto, titulado <<Simon Dice>>, consiste en el desarrollo de una versión moderna e interactiva del clásico juego de memoria <<Simón Dice>>. La innovación principal radica en la interfaz de usuario: en lugar de utilizar botones físicos o periféricos convencionales (teclado/ratón), el juego se controla enteramente mediante \textbf{visión artificial (Computer Vision)}.

El sistema es capaz de reconocer gestos faciales y manuales del jugador en tiempo real a través de una webcam, interpretándolos como comandos de juego. Esto permite una experiencia inmersiva donde el jugador debe replicar secuencias de movimientos corporales.

Este enfoque se enmarca dentro de las \textbf{interfaces naturales de usuario (NUI)}, que buscan eliminar la barrera física entre la persona y la máquina, permitiendo una interacción más intuitiva y fluida basada en las capacidades motrices humanas naturales.

\subsection{Objetivos}
\begin{itemize}
    \item Implementar un sistema de reconocimiento de gestos robusto utilizando técnicas de aprendizaje automático (Machine Learning).
    \item Desarrollar una lógica de juego interactiva que responda en tiempo real a las acciones del usuario.
    \item Integrar librerías de visión por computador como OpenCV y MediaPipe para la extracción de características.
    \item Entrenar clasificadores neuronales (MLP) para distinguir entre diferentes gestos (cara y manos).
\end{itemize}

\newpage

% 2. TECNOLOGÍAS UTILIZADAS
\section{Tecnologías y herramientas}

Para el desarrollo de este proyecto se ha utilizado un stack tecnológico basado en Python, aprovechando su extenso ecosistema para ciencia de datos y visión artificial.

\subsection{Lenguaje y entorno}
\begin{itemize}
    \item \textbf{Python 3.11}: Lenguaje principal del proyecto.
    \item \textbf{Conda}: Gestor de entornos utilizado para aislar las dependencias (entorno \texttt{VC\_Trabajo} o \texttt{memo\_env}).
    \item \textbf{Jupyter Notebooks}: Utilizados para el prototipado, generación de datasets y entrenamiento de modelos.
\end{itemize}

\subsection{Librerías principales}
\begin{itemize}
    \item \textbf{OpenCV (cv2)}: Herramienta fundamental para la captura de vídeo desde la webcam, procesamiento de imágenes (cambios de espacio de color BGR a RGB) y visualización en tiempo real.
    \item \textbf{MediaPipe}: Framework desarrollado por Google, utilizado para la detección de hitos (landmarks) faciales y manuales. Específicamente se usan los módulos \texttt{FaceMesh} (malla facial) y \texttt{Hands} (detección de manos).
    \item \textbf{Scikit-Learn}: Utilizada para la implementación de algoritmos de Machine Learning. En concreto, se ha empleado el \texttt{MLPClassifier} (Perceptrón Multicapa) para la clasificación de los gestos basándose en los landmarks extraídos.
    \item \textbf{NumPy}: Para operaciones matemáticas eficientes y manipulación de arrays, crucial para la normalización de coordenadas.
    \item \textbf{Pickle}: Para la serialización y guardado de los modelos entrenados (\texttt{.pkl}).
\end{itemize}

\newpage

% 3. ESTRUCTURA DEL REPOSITORIO
\section{Estructura del repositorio}
El proyecto se organiza en la siguiente estructura de directorios, separando el código fuente (\texttt{src}), los datos (\texttt{dataset}) y la documentación:

\begin{verbatim}
/
|-- dataset/                # Imágenes para entrenamiento
|   |-- gestos_cara/        # Dataset de gestos faciales
|   |-- gestos_manos/       # Dataset de gestos manuales
|
|-- src/                    # Código fuente del proyecto
|   |-- game/               # Lógica del juego (Simon Says)
|   |-- generate_images/    # Scripts para captura de datos
|   |-- memory/             # Documentación LaTeX
|   |-- scripts/            # Utilidades auxiliares
|   |-- train/              # Notebooks de entrenamiento (MLP)
|
|-- README.md               # Documentación general y setup
\end{verbatim}

\newpage

% 4. METODOLOGÍA Y DESARROLLO
\section{Metodología y desarrollo}

El proyecto se divide en tres fases principales: generación del dataset, entrenamiento de modelos y lógica del juego.
\subsection{Fase 1: Generación del dataset}
Para la construcción del \textit{dataset}, inicialmente se realizó una búsqueda de imágenes en repositorios públicos como Kaggle. Se identificaron conjuntos de datos relevantes, tales como:
\begin{itemize}
    \item \href{https://www.kaggle.com/datasets/ahamedfarouk/cew-dataset}{CEW Dataset} (Closed Eyes in the Wild), útil para la detección de ojos cerrados.
    \item \href{https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p}{HaGRID} (HAnd Gesture Recognition Image Dataset), para el reconocimiento de posturas de la mano.
\end{itemize}

Sin embargo, para adaptar el sistema a las necesidades específicas del juego y mejorar la robustez ante el entorno real de uso, se desarrolló un código específico para enriquecer el conjunto de datos. Mediante los scripts \texttt{GuardarGestosCara.ipynb} y \texttt{GuardarGestosManoCuerpo.ipynb}, se capturaron imágenes propias utilizando la webcam, almacenándolas en carpetas etiquetadas para complementar las muestras obtenidas de fuentes externas.

Las clases definidas para el reconocimiento incluyen:
\begin{itemize}
    \item \textbf{Gestos faciales}: Neutro, ojos cerrados, cabeza derecha y cabeza izquierda.
    \item \textbf{Gestos manuales}: Mano arriba (izquierda/derecha), puños cerrados, pulgar arriba, señal de victoria, gesto de rock, llamada y OK.
\end{itemize}

\subsection{Fase 2: Procesamiento y entrenamiento}
El entrenamiento se realiza en los notebooks \texttt{EntrenoCara.ipynb} y \texttt{EntrenoManoCuerpo.ipynb}. El flujo de trabajo es el siguiente:

En lugar de procesar la imagen cruda (píxeles), lo cual sería costoso computacionalmente y propenso a errores por cambios de iluminación, se utilizan los \textbf{landmarks} (puntos clave) de MediaPipe. Esta decisión permite una inferencia mucho más rápida, esencial para la fluidez del juego.
\begin{itemize}
    \item \textbf{Malla facial (Face Mesh)}: Genera una malla densa de 468 puntos tridimensionales que mapean la superficie del rostro. Esto permite detectar sutilezas como el cierre de los ojos o la rotación de la cabeza con gran precisión.
    \item \textbf{Manos (Hands)}: Identifica 21 puntos clave por cada mano, incluyendo muñeca y falanges de cada dedo. La topología de esqueleto permite inferir gestos complejos independientemente del tamaño de la mano del usuario.
\end{itemize}

\subsubsection{Preprocesamiento y normalización}
Los puntos extraídos ($x, y, z$) se normalizan para que el modelo sea invariante a la posición del usuario en la pantalla y a la distancia a la cámara.
\begin{lstlisting}[language=Python, caption=Función de normalización]
def normalizar_puntos(landmarks):
    coords = np.array([[lm.x, lm.y] for lm in landmarks])
    centroid = np.mean(coords, axis=0) # Centrar
    centered = coords - centroid
    max_dist = np.max(np.abs(centered)) # Escalar
    return (centered / max_dist).flatten()
\end{lstlisting}

\subsubsection{Data augmentation (aumento de datos)}
Para mejorar la robustez del modelo con pocos datos, se generan variaciones sintéticas de cada muestra original:
\begin{itemize}
    \item \textbf{Ruido Gaussiano}: Simula imperfecciones en la detección.
    \item \textbf{Escalado}: Simula ligeros acercamientos o alejamientos.
    \item \textbf{Rotación}: Simula inclinaciones de la cabeza o mano.
\end{itemize}

\subsubsection{Modelo de clasificación}
Se utiliza una red neuronal artificial (MLP).
\begin{itemize}
    \item \textbf{Arquitectura cara}: Capas ocultas de (128, 64) neuronas.
    \item \textbf{Arquitectura manos}: Capas ocultas de (64, 32) neuronas.
    \item \textbf{Validación}: Se utiliza Cross-Validation (K-Fold estratificado) para asegurar que el modelo generaliza correctamente, obteniendo altas tasas de precisión en las pruebas.
\end{itemize}

\subsection{Fase 3: Lógica del juego (Simon Says)}
El núcleo del juego se encuentra en \texttt{src/game/game.ipynb}. El sistema funciona como una máquina de estados:
\begin{enumerate}
    \item \textbf{Generación de secuencia}: El sistema elige una serie de gestos aleatorios.
    \item \textbf{Muestra}: Se indica al usuario qué gestos debe realizar (mediante texto o iconos en pantalla).
    \item \textbf{Escucha (inferencia)}: Se activa la cámara y el modelo predice en tiempo real qué gesto está haciendo el usuario.
    \item \textbf{Validación}: Si el gesto coincide con el esperado en la secuencia, se avanza. Si falla o tarda demasiado, pierde.
\end{enumerate}

\textbf{Mecánicas adicionales:}
\begin{itemize}
    \item \textbf{Trampa <<Simón Dice>>}: El juego puede intentar engañar al usuario mostrando instrucciones con un nombre diferente (ej. <<Modesto dice>>). Si el usuario realiza el gesto cuando no lo dijo <<Simón>>, pierde.
    \item \textbf{Dificultad incremental}: Aumenta la longitud de la secuencia y disminuye el tiempo permitido por ronda.
\end{itemize}

\newpage

% 5. DESAFÍOS TÉCNICOS
\section{Desafíos técnicos y soluciones}

Durante el desarrollo del proyecto surgieron varios retos inherentes a la aplicación de visión por computador en entornos no controlados:

\subsection{Variabilidad en la iluminación}
Los algoritmos de detección visual son sensibles a las condiciones de luz extremas (contraluz o oscuridad). \\
\textbf{Solución}: La normalización de las coordenadas de los landmarks hace que el modelo dependa de la geometría relativa de los puntos y no de los valores de intensidad de los píxeles, mitigando parcialmente este problema.

\subsection{Oclusiones parciales}
En ocasiones, al realizar gestos cerca de la cara, las manos pueden ocultar partes del rostro, confundiendo al detector facial. \\
\textbf{Solución}: Se implementó una lógica de priorización en el bucle principal del juego y se ajustaron los umbrales de confianza (\texttt{min\_detection\_confidence}) para reducir falsos positivos.

\subsection{Latencia en tiempo real}
El procesamiento de vídeo requiere un alto rendimiento para no afectar a la jugabilidad. \\
\textbf{Solución}: El uso de clasificadores MLP (redes neuronales simples) sobre vectores de características ligeros, en lugar de redes convolucionales profundas (CNN) sobre imágenes completas, garantiza una tasa de fotogramas (FPS) alta incluso en equipos con hardware modesto.

\newpage

% 6. MANUAL DE USUARIO
\section{Manual de usuario}

\subsection{Requisitos previos}
Es necesario disponer de una webcam funcional y un entorno con las librerías instaladas:
\begin{verbatim}
pip install opencv-python numpy mediapipe scikit-learn
\end{verbatim}

\subsection{Ejecución}
1. Abrir el proyecto en un editor compatible con Jupyter Notebooks (VS Code o Jupyter Lab).
2. Ejecutar el archivo principal del juego: \texttt{src/game/game.ipynb}.
3. Seguir las instrucciones en pantalla.

\subsection{Cómo jugar}
\begin{enumerate}
    \item El juego mostrará una secuencia de acciones (ej. <<Cerrar Ojos>>, <<Mano Arriba>>).
    \item Memoriza la secuencia.
    \item Cuando sea tu turno, repite los gestos frente a la cámara en el mismo orden.
    \item ¡Cuidado! Si la instrucción no empieza por <<Simón dice>> (o la indicación visual correspondiente), no debes moverte.
\end{enumerate}

\newpage

% 7. CONCLUSIONES
\section{Conclusiones}
Este proyecto demuestra la viabilidad de crear interfaces hombre-máquina naturales y accesibles utilizando hardware de consumo estándar (webcam). La combinación de la extracción de características geométricas (mediante MediaPipe) con clasificadores ligeros (MLP) permite una inferencia en tiempo real extremadamente rápida, adecuada para videojuegos.

El uso de técnicas como la normalización de coordenadas y el aumento de datos ha sido crucial para obtener un sistema robusto que funcione con diferentes usuarios y condiciones de iluminación.

\newpage

% 8. BIBLIOGRAFÍA
\section{Bibliografía}
\begin{itemize}
    \item Documentación de OpenCV: \url{https://docs.opencv.org/}
    \item MediaPipe Solutions: \url{https://google.github.io/mediapipe/}
    \item Scikit-Learn Documentation: \url{https://scikit-learn.org/stable/}
    \item Ultralytics YOLO (referencia conceptual): \url{https://github.com/ultralytics/ultralytics}
\end{itemize}

\end{document}
